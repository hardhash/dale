****
**吐槽** 我必须要吐槽一下，我硕士毕业是2023年，秋招是2022年9月参加的，当时NLP还是各种BERT打天下，kaggle上nlp竞赛也是BERT家族+各种魔改mlp层。那会儿也是hugging face，但是，但是，但是！现在2023年，Transformers似乎一统江湖了，以前写半天的代码现在都是框架复用。而且RNN，LSTM，GRU这些当时风靡一时高大上的循环神经网络，现在看起来跟2-Gram没什么区别，bert+bilstm+crf多么成熟的方案，哎，都是时代的眼泪，版本的弃子（哈哈开玩笑）。
随着ChatGPT火出圈，不会llm似乎根本找不到工作，才过了一年，游戏直接重开，有点像三体游戏直接按核按钮给星球毁灭了，好家伙，既然这样，我就把llm东西给他加进来！紧跟时代！
****

> > 什么是NLP

NLP( Nature Language Processing)，主要关注人类语言和机器语言的交互。例如自动生成李白风格的诗歌、机器翻译、完形填空、阅读理解等等。在1950年计算机科学之父图灵论文提出“Computing machinery and intelligence”开始，人类语言就开始尝试和计算机科学进行交融。回想一下我们在学习英语的时候，是不是买了语法书，上语法课，知道什么是主谓宾定状补，什么动词放在名词后，一个句子只能有一个动词等等。没错，在NLP初期，很多人也是通过类似的方式，基于规则的方式，教给计算机语法，从语法的角度去理解人类语言。但是这样有一个最大的弊端，语法是一个非常抽象复杂的东西，而语法和理解语言真的有必然的联系吗？或者说精通英文语法和精通英文理解的联系大吗？例如听到“Welcome to China”你大脑会去分析这一句是一个祈使句没有主语这种语法结构，才能理解他的意思吗？显然不是，语法只是语言学家从自然语言中总结出来的片面的规则，他总有缺陷。但最开始基于统计效果也很一般，因为没有很好的概率统计模型去完成这个工作，到21世纪，深度学习技术兴起，迅速占领NLP主流市场，各种语音助手、机器翻译开始出现，基于规则的流派逐渐成为江湖传说。

> 章节安排

NLP部分直到WaveNet章节，都是2023年以前的学习资料。按理说，这部分的内容足以应对足够多的下游场景。可是2023年又是大模型集中火热出圈的一年，大模型改写了许多底层任务逻辑，让多模态更容易实现，因此，前面部分视作NLP基础内容，即循环神经网络和注意力机制。这个时期都是NLP任务的第二阶段，即简单神经网络+大量数据自己训练。

2018-2022年，NLP任务多可通过hugging face下载比较不错的bert模型微调，应对下游任务，在竞赛总结章节的Kaggle和讯飞两场比赛都是直接下载的对应模型，借助相应的一些trick手段取得不错的成绩。我们把这个时代叫做NLP发展的第三阶段，即神经网络+预训练模型+少量数据。

如今（截至2023年），NLP已经进入第四阶段，即神经网络+更大的预训练模型+Prompt。与此同时，NLP大部分任务的实现依然是通过hugging face的Transformers实现，但是由于大模型的引入，Transformers也进行了较多更新，因此需要补充学习Transformer知识，满足大模型和多模态任务的当代需求。（**这里的Transformers不是注意力机制的transformer架构，而是hugging face的API工具包**）